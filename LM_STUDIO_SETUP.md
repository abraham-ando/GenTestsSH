# Guide de Configuration LM Studio

## üéØ Configuration Compl√®te pour Utiliser LM Studio

Votre projet est maintenant configur√© pour utiliser **LM Studio** avec le mod√®le **openai/gpt-oss-20b** en local !

---

## ‚úÖ √âtape 1: V√©rifier LM Studio

### Lancer LM Studio
1. Ouvrez **LM Studio**
2. Assurez-vous que le mod√®le `openai/gpt-oss-20b` est bien t√©l√©charg√©
3. Chargez le mod√®le dans LM Studio

### Activer le Serveur Local
1. Dans LM Studio, allez dans l'onglet **"Local Server"** ou **"Developer"**
2. Cliquez sur **"Start Server"**
3. V√©rifiez que le serveur d√©marre sur `http://localhost:1234`
4. Le mod√®le devrait √™tre pr√™t √† recevoir des requ√™tes

---

## ‚úÖ √âtape 2: Configuration du Projet (D√âJ√Ä FAIT ‚úì)

Votre fichier `.env` a √©t√© automatiquement mis √† jour avec :

```env
# LM Studio Configuration (Local Model)
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=openai/gpt-oss-20b
OPENAI_BASE_URL=http://localhost:1234/v1

# LLM Provider
LLM_PROVIDER=openai
```

**Points importants :**
- ‚úÖ `OPENAI_API_KEY=lm-studio` : Cl√© factice (LM Studio n'en a pas besoin)
- ‚úÖ `OPENAI_MODEL=openai/gpt-oss-20b` : Votre mod√®le local
- ‚úÖ `OPENAI_BASE_URL=http://localhost:1234/v1` : URL du serveur LM Studio
- ‚úÖ `LLM_PROVIDER=openai` : Utilise l'API compatible OpenAI

---

## ‚úÖ √âtape 3: Modifications du Code (D√âJ√Ä FAIT ‚úì)

J'ai mis √† jour automatiquement :

### 1. `config.py`
- Ajout du champ `openai_base_url` pour supporter LM Studio

### 2. `llm_analyzer.py`
- D√©tection automatique de LM Studio via `OPENAI_BASE_URL`
- Configuration de `openai.api_base` pour pointer vers votre serveur local
- Logs informatifs pour confirmer l'utilisation de LM Studio

---

## üöÄ √âtape 4: Tester la Configuration

### Test 1: V√©rifier la configuration
```bash
python sources\tests\playwright\cli.py config-check
```

Vous devriez voir :
```
‚úì LM Studio mode detected - using local model
‚úì Model configured: openai/gpt-oss-20b
‚úì Using custom OpenAI base URL: http://localhost:1234/v1
```

### Test 2: Lancer un test simple
```bash
python sources\tests\playwright\test_runner.py
```

### Test 3: Lancer la suite compl√®te
```bash
pytest sources\tests\playwright\main.py -v
```

---

## üîç V√©rification du Serveur LM Studio

### Test manuel de l'API
Ouvrez PowerShell et ex√©cutez :

```powershell
# Test de sant√© du serveur
curl http://localhost:1234/v1/models

# Devrait retourner quelque chose comme :
# {"data": [{"id": "openai/gpt-oss-20b", ...}]}
```

Ou avec Python :
```python
import requests
response = requests.get("http://localhost:1234/v1/models")
print(response.json())
```

---

## ‚öôÔ∏è Param√®tres LM Studio Recommand√©s

Dans LM Studio, pour le mod√®le GPT-OSS-20B :

### Param√®tres de G√©n√©ration
- **Temperature**: 0.0 - 0.2 (pour des r√©ponses d√©terministes)
- **Max Tokens**: 1000 - 2000
- **Top P**: 0.9
- **Context Length**: 4096 ou plus

### Param√®tres Serveur
- **Port**: 1234 (par d√©faut)
- **CORS**: Activ√© si n√©cessaire
- **API Key**: Pas n√©cessaire pour local

---

## üéØ Avantages de LM Studio

‚úÖ **Gratuit** - Pas de frais API  
‚úÖ **Priv√©** - Donn√©es restent en local  
‚úÖ **Rapide** - Pas de latence r√©seau  
‚úÖ **Offline** - Fonctionne sans Internet  
‚úÖ **Illimit√©** - Pas de limite de requ√™tes  

---

## üêõ D√©pannage

### Erreur: "Connection refused"
**Solution**: V√©rifiez que le serveur LM Studio est d√©marr√©
```bash
# Dans LM Studio: Start Server
```

### Erreur: "Model not found"
**Solution**: V√©rifiez que le nom du mod√®le dans `.env` correspond exactement √† celui dans LM Studio
```env
OPENAI_MODEL=openai/gpt-oss-20b  # Doit correspondre exactement
```

### Erreur: "Timeout"
**Solution**: Le mod√®le 20B peut √™tre lent. Augmentez le timeout :
```env
TIMEOUT=60000  # 60 secondes au lieu de 30
```

### Port diff√©rent de 1234
**Solution**: Si LM Studio utilise un autre port, mettez √† jour :
```env
OPENAI_BASE_URL=http://localhost:VOTRE_PORT/v1
```

---

## üìä Comparaison des Performances

| Crit√®re | LM Studio (Local) | OpenAI API |
|---------|-------------------|------------|
| Co√ªt | Gratuit ‚úÖ | ~$0.001/requ√™te |
| Vitesse | 2-5s (selon GPU) | 1-2s |
| Confidentialit√© | 100% priv√© ‚úÖ | Cloud |
| Disponibilit√© | Offline ‚úÖ | Internet requis |
| Qualit√© | Bon (20B params) | Excellent (GPT-4) |

---

## üîÑ Basculer entre LM Studio et OpenAI

### Pour utiliser LM Studio (actuel)
```env
LLM_PROVIDER=openai
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=openai/gpt-oss-20b
OPENAI_BASE_URL=http://localhost:1234/v1
```

### Pour revenir √† OpenAI API
```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-votre-vraie-cle-api
OPENAI_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=  # Commenter ou laisser vide
```

### Pour utiliser Anthropic Claude
```env
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=votre-cle-anthropic
ANTHROPIC_MODEL=claude-3-sonnet-20240229
```

---

## ‚úÖ Checklist de Configuration

- [x] LM Studio install√©
- [x] Mod√®le `openai/gpt-oss-20b` t√©l√©charg√©
- [ ] **Serveur LM Studio d√©marr√©** ‚ö†Ô∏è (√Ä FAIRE MAINTENANT)
- [x] Fichier `.env` configur√©
- [x] Code mis √† jour pour supporter LM Studio
- [ ] Test de connexion r√©ussi
- [ ] Premier test auto-heal ex√©cut√©

---

## üöÄ Prochaine √âtape

**MAINTENANT:** Lancez le serveur LM Studio et testez !

```bash
# 1. Dans LM Studio: Start Server (port 1234)

# 2. V√©rifiez la configuration
python sources\tests\playwright\cli.py config-check

# 3. Lancez un test
python sources\tests\playwright\test_runner.py
```

---

**Vous √™tes pr√™t !** Le framework utilisera maintenant votre mod√®le local GPT-OSS-20B au lieu de l'API OpenAI. üéâ

---

**Note**: Si vous avez des questions ou rencontrez des probl√®mes, consultez la section D√©pannage ci-dessus.

